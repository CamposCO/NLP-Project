{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/vscode/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package punkt to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/vscode/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, confusion_matrix\n",
    "from sklearn.metrics import f1_score  #ytrue, ypred\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# NLP toolkits\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_SVC(X_train, y_train, performance_metric='f1', resultsGrid=False):\n",
    "    model = SVC()\n",
    "    C = np.linspace(0.000001 , 1000, 10)\n",
    "    kernels = ['poly', 'rbf', 'linear', 'sigmoid']\n",
    "    gamma = ['scale', 'auto']\n",
    "    grid = dict(C = C, kernel = kernels, gamma = gamma)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv,\n",
    "                           scoring=performance_metric,error_score='raise')\n",
    "    grid_result = grid_search.fit(X_train, y_train)\n",
    "    if resultsGrid==True:\n",
    "        return grid_result.cv_results_\n",
    "    else:\n",
    "        return  grid_result.best_estimator_\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() # active\n",
    "\n",
    "def lemmatize_text(text): #Lematizaci√≥n del texto.\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "    return lemmatized_text\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(string):\n",
    "    string = string.lower()\n",
    "    string = re.sub(r\"http(s)?:*\", '', string)\n",
    "    string = re.sub(r\"[-/.#&]\", ' ', string)\n",
    "    string = re.sub(r\"w{3}\", ' ', string)\n",
    "    string = string.strip()\n",
    "    string = ' '.join([word for word in string.split() if word not in stop_words])\n",
    "    string = lemmatize_text(string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/4GeeksAcademy/NLP-project-tutorial/main/url_spam.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['url', 'is_spam'], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [df['url'].loc[np.random.randint(0,df.shape[0])] for _ in range(30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vanityfair com hollywood 2020 06 ziwe fumudoh baited interview alison roman',\n",
       " 'briefingday com v4n3i4f3',\n",
       " 'itsmestevebryant com',\n",
       " 'propublica org article removed label said medical use prohibited tried sell thousand mask official distribute hospital 970141',\n",
       " 'mailchi mp bigspaceship big spaceship internet brunch 1729154',\n",
       " 'theguardian com tv radio 2020 jun 24 netflixs floor lava show save summer',\n",
       " 'cnet com news dark matter detector pick unexplained unexpected signal',\n",
       " 'nbcnews com health health news cdc say covid 19 case u may 10 n1232134',\n",
       " 'youtube com watch ? v=zo0ssjb1tri utm_source=morning_brew',\n",
       " 'thehustle co 06302020 foraging mushroom',\n",
       " 'wired com story algorithm predicts criminality based face spark furor',\n",
       " 'espn com nfl story _ id 29383371 netflix produce 6 part series colin kaepernick',\n",
       " 'youtube com watch ? v=l3qqqu7qlom feature=emb_title',\n",
       " 'nytimes com interactive 2020 06 24 magazine reparation slavery html',\n",
       " 'cnn com 2020 06 23 tech wirecard ceo markus braun arrested index html',\n",
       " 'visualcapitalist com billion dollar transfer military equipment police department',\n",
       " 'join1440 com',\n",
       " 'nytimes com interactive 2020 06 26 art design rosie lee tompkins quilt html',\n",
       " 'nytimes com interactive 2019 08 14 magazine 1619 america slavery html',\n",
       " 'mybillie com page skimm',\n",
       " 'reuters com article u usa auto fleet sale analysis slumping fleet sale weigh u auto market iduskbn2411nh',\n",
       " 'morningbrew com daily story 2020 05 31 spacex successfully launch human space',\n",
       " 'morningbrew com daily story 2020 06 28 controversy mask threatens u pandemic response',\n",
       " 'cnbc com 2020 06 26 amazon spending 1 billion zoox invest billion html',\n",
       " 'explore org livecams brown bear brown bear salmon cam brook fall',\n",
       " 'economist com science technology 2020 06 25 pandemic proofing planet',\n",
       " 'nbcnews com news u news golden state killer suspect plead guilty 13 count first degree n1232395',\n",
       " 'vox com culture 21294429 eurovision review interview ferrell rachel mcadams netflix',\n",
       " 'morningbrew com daily story 2020 06 21 whats happening wirecard',\n",
       " 'youtube com channel ucyan6mg5u8cjy2zi4ikwaug']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer() # active\n",
    "stop_words.extend(['of', 'yet']) # we can add stop words\n",
    "list(map(clean_text, samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df['url'], df['is_spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                    shuffle=True,\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning data\n",
    "X_train = X_train.apply(lambda x : clean_text(x))\n",
    "X_test = X_test.apply(lambda x : clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer().fit(X_train)\n",
    "#vect = TfidfVectorizer().fit(X_train)\n",
    "X_train = vect.transform(X_train)\n",
    "X_test  = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where(y_train==True,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.96      0.97       682\n",
      "        True       0.87      0.92      0.89       218\n",
      "\n",
      "    accuracy                           0.95       900\n",
      "   macro avg       0.92      0.94      0.93       900\n",
      "weighted avg       0.95      0.95      0.95       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_ml = grid_SVC(X_train, y_train)\n",
    "preds = best_ml.predict(X_test)\n",
    "print(classification_report(y_test, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
